{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from ddpg_agent import Agent\n",
    "from buffer import ReplayBuffer\n",
    "from collections import deque,namedtuple\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from maddpg_agent import MADDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.65278625 -1.5\n",
      "  -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.4669857  -1.5\n",
      "   0.          0.         -6.83172083  6.          0.          0.        ]]\n",
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode = False)[brain_name]\n",
    "print(env_info.vector_observations)\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00334968 1.        ]\n",
      " [0.10860023 0.56719579]]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -7.38993645 -1.5        -0.          0.\n",
      "   6.83172083  5.99607611 -0.          0.         -7.3798871  -0.98316395\n",
      "   0.10049034  6.21520042  6.83172083  5.91759634  0.10049034  6.21520042]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -6.70024681 -1.5         0.          0.\n",
      "  -6.83172083  5.99607611  0.          0.         -6.37444544 -0.98316395\n",
      "   3.25800681  6.21520042 -6.83172083  5.91759634  3.25800681  6.21520042]]\n",
      "[0.0, 0.0]\n",
      "[False, False]\n",
      "[[ 0.14187236  0.52576424]\n",
      " [-0.99707873 -0.61814165]]\n",
      "[[ -7.38993645  -1.5         -0.           0.           6.83172083\n",
      "    5.99607611  -0.           0.          -7.3798871   -0.98316395\n",
      "    0.10049034   6.21520042   6.83172083   5.91759634   0.10049034\n",
      "    6.21520042  -6.95426941  -0.42050385   4.25617075   5.23420095\n",
      "    6.83172083   5.74101639   4.25617075   5.23420095]\n",
      " [ -6.70024681  -1.5          0.           0.          -6.83172083\n",
      "    5.99607611   0.           0.          -6.37444544  -0.98316395\n",
      "    3.25800681   6.21520042  -6.83172083   5.91759634   3.25800681\n",
      "    6.21520042  -9.3656826   -0.42050385 -29.91236115   5.23420095\n",
      "   -6.83172083   5.74101639 -29.91236115   5.23420095]]\n",
      "[0.0, 0.0]\n",
      "[False, False]\n",
      "[[ 0.04692321  0.88289075]\n",
      " [-1.          0.57216507]]\n",
      "[[ -7.3798871   -0.98316395   0.10049034   6.21520042   6.83172083\n",
      "    5.91759634   0.10049034   6.21520042  -6.95426941  -0.42050385\n",
      "    4.25617075   5.23420095   6.83172083   5.74101639   4.25617075\n",
      "    5.23420095  -6.81349993   0.0440563    1.40769637   4.25320148\n",
      "    6.83172083   5.46633625   1.40769637   4.25320148]\n",
      " [ -6.37444544  -0.98316395   3.25800681   6.21520042  -6.83172083\n",
      "    5.91759634   3.25800681   6.21520042  -9.3656826   -0.42050385\n",
      "  -29.91236115   5.23420095  -6.83172083   5.74101639 -29.91236115\n",
      "    5.23420095 -10.91042614   0.0440563    0.           4.25320148\n",
      "   -6.83172083   5.46633625   0.           4.25320148]]\n",
      "[0.0, 0.0]\n",
      "[False, False]\n",
      "[[ 0.36602359 -0.4666113 ]\n",
      " [ 0.92360639 -0.83505055]]\n",
      "[[ -6.95426941  -0.42050385   4.25617075   5.23420095   6.83172083\n",
      "    5.74101639   4.25617075   5.23420095  -6.81349993   0.0440563\n",
      "    1.40769637   4.25320148   6.83172083   5.46633625   1.40769637\n",
      "    4.25320148  -5.71542978   0.41051644  10.98070812   3.27220201\n",
      "    6.83172083   5.0935564   10.98070812   3.27220201]\n",
      " [ -9.3656826   -0.42050385 -29.91236115   5.23420095  -6.83172083\n",
      "    5.74101639 -29.91236115   5.23420095 -10.91042614   0.0440563\n",
      "    0.           4.25320148  -6.83172083   5.46633625   0.\n",
      "    4.25320148  -8.13960648   0.41051644  27.70819092   3.27220201\n",
      "   -6.83172083   5.0935564   27.70819092   3.27220201]]\n",
      "[0.0, 0.0]\n",
      "[False, False]\n",
      "[[ 0.3153436  -0.05787763]\n",
      " [ 0.0687365  -0.78442418]]\n",
      "[[ -6.81349993   0.0440563    1.40769637   4.25320148   6.83172083\n",
      "    5.46633625   1.40769637   4.25320148  -5.71542978   0.41051644\n",
      "   10.98070812   3.27220201   6.83172083   5.0935564   10.98070812\n",
      "    3.27220201  -4.76939917   0.67887664   9.46030807   2.29120255\n",
      "    6.83172083   4.62267637   9.46030807   2.29120255]\n",
      " [-10.91042614   0.0440563    0.           4.25320148  -6.83172083\n",
      "    5.46633625   0.           4.25320148  -8.13960648   0.41051644\n",
      "   27.70819092   3.27220201  -6.83172083   5.0935564   27.70819092\n",
      "    3.27220201  -7.93339777   0.67887664   2.06209493   2.29120255\n",
      "   -6.83172083   4.62267637   2.06209493   2.29120255]]\n",
      "[0.0, 0.0]\n",
      "[False, False]\n",
      "[[-0.32394702  0.50465018]\n",
      " [ 1.         -0.08757395]]\n",
      "[[-5.71542978  0.41051644 10.98070812  3.27220201  6.83172083  5.0935564\n",
      "  10.98070812  3.27220201 -4.76939917  0.67887664  9.46030807  2.29120255\n",
      "   6.83172083  4.62267637  9.46030807  2.29120255 -5.74124098  0.84913683\n",
      "  -9.71841049  1.3102026   6.83172083  4.05369663 -9.71841049  1.3102026 ]\n",
      " [-8.13960648  0.41051644 27.70819092  3.27220201 -6.83172083  5.0935564\n",
      "  27.70819092  3.27220201 -7.93339777  0.67887664  2.06209493  2.29120255\n",
      "  -6.83172083  4.62267637  2.06209493  2.29120255 -4.93339825  0.84913683\n",
      "  30.          1.3102026  -6.83172083  4.05369663 30.          1.3102026 ]]\n",
      "[0.0, 0.0]\n",
      "[False, False]\n",
      "[[-0.2250071 -1.       ]\n",
      " [-1.         1.       ]]\n",
      "[[ -4.76939917   0.67887664   9.46030807   2.29120255   6.83172083\n",
      "    4.62267637   9.46030807   2.29120255  -5.74124098   0.84913683\n",
      "   -9.71841049   1.3102026    6.83172083   4.05369663  -9.71841049\n",
      "    1.3102026   -6.4162612    0.92129707  -6.75021315   0.32920253\n",
      "    6.83172083   3.38661671  -6.75021315   0.32920253]\n",
      " [ -7.93339777   0.67887664   2.06209493   2.29120255  -6.83172083\n",
      "    4.62267637   2.06209493   2.29120255  -4.93339825   0.84913683\n",
      "   30.           1.3102026   -6.83172083   4.05369663  30.\n",
      "    1.3102026   -7.93339777   0.92129707 -30.           0.32920253\n",
      "   -6.83172083   3.38661671 -30.           0.32920253]]\n",
      "[0.0, 0.0]\n",
      "[False, False]\n",
      "[[-1.          0.65336828]\n",
      " [ 1.          0.05749798]]\n",
      "[[ -5.74124098   0.84913683  -9.71841049   1.3102026    6.83172083\n",
      "    4.05369663  -9.71841049   1.3102026   -6.4162612    0.92129707\n",
      "   -6.75021315   0.32920253   6.83172083   3.38661671  -6.75021315\n",
      "    0.32920253  -9.41626263   0.89535731 -30.          -0.65179747\n",
      "    6.83172083   2.62143707 -30.          -0.65179747]\n",
      " [ -4.93339825   0.84913683  30.           1.3102026   -6.83172083\n",
      "    4.05369663  30.           1.3102026   -7.93339777   0.92129707\n",
      "  -30.           0.32920253  -6.83172083   3.38661671 -30.\n",
      "    0.32920253  -4.93339825   0.89535731  30.          -0.65179747\n",
      "   -6.83172083   2.62143707  30.          -0.65179747]]\n",
      "[0.0, 0.0]\n",
      "[False, False]\n",
      "[[ 1.         -1.        ]\n",
      " [ 0.02631274  0.47759656]]\n",
      "[[ -6.4162612    0.92129707  -6.75021315   0.32920253   6.83172083\n",
      "    3.38661671  -6.75021315   0.32920253  -9.41626263   0.89535731\n",
      "  -30.          -0.65179747   6.83172083   2.62143707 -30.\n",
      "   -0.65179747  -6.41626167   0.7713176   30.          -1.63279748\n",
      "    6.83172083   1.75815713  30.          -1.63279748]\n",
      " [ -7.93339777   0.92129707 -30.           0.32920253  -6.83172083\n",
      "    3.38661671 -30.           0.32920253  -4.93339825   0.89535731\n",
      "   30.          -0.65179747  -6.83172083   2.62143707  30.\n",
      "   -0.65179747  -4.85446024   0.7713176    0.78938228  -1.63279748\n",
      "   -6.83172083   1.75815713   0.78938228  -1.63279748]]\n",
      "[0.0, 0.0]\n",
      "[False, False]\n",
      "[[ 0.34042133 -0.03348475]\n",
      " [ 0.2878413   0.43305179]]\n",
      "[[ -9.41626263   0.89535731 -30.          -0.65179747   6.83172083\n",
      "    2.62143707 -30.          -0.65179747  -6.41626167   0.7713176\n",
      "   30.          -1.63279748   6.83172083   1.75815713  30.\n",
      "   -1.63279748  -5.39499807   0.54917783  10.21263981  -2.61379719\n",
      "    6.83172083   0.83853728  10.21263981  -2.61379719]\n",
      " [ -4.93339825   0.89535731  30.          -0.65179747  -6.83172083\n",
      "    2.62143707  30.          -0.65179747  -4.85446024   0.7713176\n",
      "    0.78938228  -1.63279748  -6.83172083   1.75815713   0.78938228\n",
      "   -1.63279748  -3.99093699   0.54917783   8.63523865  -2.61379719\n",
      "   -6.83172083   0.83853728   8.63523865  -2.61379719]]\n",
      "[0.0, 0.0]\n",
      "[False, False]\n",
      "[[-0.08131489 -1.        ]\n",
      " [-0.03844082  0.46006965]]\n",
      "[[-6.41626167  0.7713176  30.         -1.63279748  6.83172083  1.75815713\n",
      "  30.         -1.63279748 -5.39499807  0.54917783 10.21263981 -2.61379719\n",
      "   6.83172083  0.83853728 10.21263981 -2.61379719 -5.6389432   0.22893813\n",
      "  -2.43944645 -3.59479666  6.83172083 -0.08108279 -2.43944645 -3.59479666]\n",
      " [-4.85446024  0.7713176   0.78938228 -1.63279748 -6.83172083  1.75815713\n",
      "   0.78938228 -1.63279748 -3.99093699  0.54917783  8.63523865 -2.61379719\n",
      "  -6.83172083  0.83853728  8.63523865 -2.61379719 -4.10625982  0.22893813\n",
      "  -1.15322447 -3.59479666 -6.83172083 -0.08108279 -1.15322447 -3.59479666]]\n",
      "[0.0, 0.0]\n",
      "[False, False]\n",
      "[[0.23270505 1.        ]\n",
      " [0.54261792 0.18697059]]\n",
      "[[-5.39499807  0.54917783 10.21263981 -2.61379719  6.83172083  0.83853728\n",
      "  10.21263981 -2.61379719 -5.6389432   0.22893813 -2.43944645 -3.59479666\n",
      "   6.83172083 -0.08108279 -2.43944645 -3.59479666 -4.94082737 -0.18940151\n",
      "   6.98115158 -4.57579613  6.83172083 -1.00070286  6.98115158 -4.57579613]\n",
      " [-3.99093699  0.54917783  8.63523865 -2.61379719 -6.83172083  0.83853728\n",
      "   8.63523865 -2.61379719 -4.10625982  0.22893813 -1.15322447 -3.59479666\n",
      "  -6.83172083 -0.08108279 -1.15322447 -3.59479666 -2.47840619 -0.18940151\n",
      "  16.27853775 -4.57579613 -6.83172083 -1.00070286 16.27853775 -4.57579613]]\n",
      "[0.0, 0.0]\n",
      "[False, False]\n",
      "[[-1.         -0.08698617]\n",
      " [ 0.56185196 -0.07951389]]\n",
      "[[ -5.6389432    0.22893813  -2.43944645  -3.59479666   6.83172083\n",
      "   -0.08108279  -2.43944645  -3.59479666  -4.94082737  -0.18940151\n",
      "    6.98115158  -4.57579613   6.83172083  -1.00070286   6.98115158\n",
      "   -4.57579613  -7.94082689  -0.70584118 -30.          -5.5567956\n",
      "    6.83172083  -1.92032266 -30.          -5.5567956 ]\n",
      " [ -4.10625982   0.22893813  -1.15322447  -3.59479666  -6.83172083\n",
      "   -0.08108279  -1.15322447  -3.59479666  -2.47840619  -0.18940151\n",
      "   16.27853775  -4.57579613  -6.83172083  -1.00070286  16.27853775\n",
      "   -4.57579613  -0.79284996  -0.70584118  16.8555603   -5.5567956\n",
      "   -6.83172083  -1.92032266  16.8555603   -5.5567956 ]]\n",
      "[0.0, 0.0]\n",
      "[False, False]\n",
      "[[-1.          0.84300849]\n",
      " [ 0.3951002   1.        ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4.94082737  -0.18940151   6.98115158  -4.57579613   6.83172083\n",
      "   -1.00070286   6.98115158  -4.57579613  -7.94082689  -0.70584118\n",
      "  -30.          -5.5567956    6.83172083  -1.92032266 -30.\n",
      "   -5.5567956  -10.94082928  -1.32038057 -30.          -6.53779507\n",
      "    6.83172083  -2.83994246 -30.          -6.53779507]\n",
      " [ -2.47840619  -0.18940151  16.27853775  -4.57579613  -6.83172083\n",
      "   -1.00070286  16.27853775  -4.57579613  -0.79284996  -0.70584118\n",
      "   16.8555603   -5.5567956   -6.83172083  -1.92032266  16.8555603\n",
      "   -5.5567956   -0.76293987  -1.32038057  11.85300636  -6.53779507\n",
      "   -6.83172083  -2.83994246  11.85300636  -6.53779507]]\n",
      "[0.0, 0.0]\n",
      "[False, False]\n",
      "[[-1.          0.41953963]\n",
      " [-0.92204188  0.06262827]]\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.43639946 -1.5\n",
      "  -0.          0.          6.69487906  5.96076012 -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.73019552 -1.5\n",
      "   0.          0.         -6.69487906  5.96076012  0.          0.        ]]\n",
      "[0.0, -0.009999999776482582]\n",
      "[True, True]\n",
      "Score (max over agents) from episode 1: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 2):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        print(actions)\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        print(next_states)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        print(rewards)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        print(dones)\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))                                        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.47879982 -0.5         0.          0.          2.23162635  1.98692004\n",
      "  0.          0.        ]\n",
      "[-2.57673184 -0.5         0.          0.         -2.23162635  1.98692004\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "def vector_avg(states):\n",
    "    state_1 = np.array(states[0])\n",
    "    state_2 = np.array(states[1])\n",
    "    [state_1_1, state_1_2, state_1_3] = np.split(state_1, 3)\n",
    "    [state_2_1, state_2_2, state_2_3] = np.split(state_2, 3)\n",
    "    state_1 = (state_1_1 + state_1_2 + state_1_3)/3\n",
    "    state_2 = (state_2_1 + state_2_2 + state_2_3)/3\n",
    "    return [state_1, state_2]\n",
    "    \n",
    "\n",
    "\n",
    "state = vector_avg(states)\n",
    "print(state[0])\n",
    "\n",
    "print(state[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 0\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 1\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 2\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 3\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 4\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 5\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 6\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 7\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 8\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 9\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 10\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 11\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 12\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 13\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 14\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 15\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 16\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 17\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 18\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 19\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 20\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 21\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 22\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 23\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 24\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 25\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 26\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 27\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 28\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 29\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 30\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 31\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 32\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 33\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 34\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 35\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 36\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 37\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 38\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 39\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 40\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 41\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 42\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 43\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 44\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 45\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 46\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 47\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 48\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 49\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 50\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 51\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 52\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 53\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 54\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 55\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 56\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 57\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 58\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 59\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 60\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 61\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 62\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 63\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 64\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 65\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 66\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 67\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 68\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 69\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 70\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 71\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 72\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 73\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 74\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 75\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 76\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      " Episode 77\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      " Episode 78\tAverage Score: 0.00[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ba99da4236e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaddpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaddpg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"checkpoint_actor_agent2.pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaddpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaddpg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"checkpoint_critic_agent2.pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-ba99da4236e9>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mmaddpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m                                  \u001b[1;31m# exit loop if episode finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Unity\\2018.4\\p3_collab-compet\\maddpg_agent.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done, Batch)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUpdate_times\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.99\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Unity\\2018.4\\p3_collab-compet\\maddpg_agent.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUpdate_times\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.99\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Unity\\2018.4\\p3_collab-compet\\maddpg_agent.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, experiences, agent_number, gamma)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoft_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoft_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msoft_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Unity\\2018.4\\p3_collab-compet\\maddpg_agent.py\u001b[0m in \u001b[0;36msoft_update\u001b[1;34m(self, local_model, target_model, tau)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msoft_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtarget_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_param\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[0mtarget_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlocal_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtarget_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "maddpg = MADDPG(24,2)\n",
    "\n",
    "def main():\n",
    "    print_every = 10\n",
    "    scores_deque = deque(maxlen = 100)\n",
    "    scores = []\n",
    "    num_episodes = 5000\n",
    "    episode_length = 80\n",
    "    batchsize = 256\n",
    "    noise = 2\n",
    "    noise_reduction = 0.9999\n",
    "    t_max = 300\n",
    "    for episode in range(0,num_episodes):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        [agent.reset() for agent in maddpg.maddpg]\n",
    "        score = 0\n",
    "        for t in range(t_max):\n",
    "            actions = maddpg.act((states), noise = noise)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            score += max(env_info.rewards)\n",
    "            print(rewards)\n",
    "            maddpg.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        print(\"\\r Episode {}\\tAverage Score: {:.2f}\".format(episode, np.mean(scores_deque)), end = \"\")\n",
    "        torch.save(maddpg.maddpg[0].actor_local.state_dict(), \"checkpoint_actor_agent1.pth\")\n",
    "        torch.save(maddpg.maddpg[0].critic_local.state_dict(), \"checkpoint_critic_agent1.pth\")\n",
    "        torch.save(maddpg.maddpg[1].actor_local.state_dict(), \"checkpoint_actor_agent2.pth\")\n",
    "        torch.save(maddpg.maddpg[1].critic_local.state_dict(), \"checkpoint_critic_agent2.pth\")\n",
    "scores = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actions)\n",
    "states = states.reshape(1,-1)\n",
    "\n",
    "print(states.reshape(1, -1).shape)\n",
    "states = states.reshape(-1, 2, 24)\n",
    "\n",
    "critic_input = np.concatenate((states[0], actions[0]), axis = None)\n",
    "print(critic_input)\n",
    "print(scores_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
